# Inference Configuration

model:
  stage: production  # production, staging, or trained
  confidence_interval: true

inference:
  batch_size: 1000
  output_format: csv

api:
  host: 0.0.0.0
  port: 8000
  workers: 4
  timeout: 60

monitoring:
  enable_logging: true
  log_predictions: true
  drift_detection: true

mlflow:
  tracking_uri: http://localhost:5000
