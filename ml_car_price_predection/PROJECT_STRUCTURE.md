# Car Price Prediction - Production ML Pipeline

## ğŸ“ Project Structure

```
ml_car_price_predection/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ data/                     # Data processing
â”‚   â”‚   â”œâ”€â”€ ingestion.py         # Data loading
â”‚   â”‚   â”œâ”€â”€ validation.py        # Data quality checks
â”‚   â”‚   â””â”€â”€ preprocessing.py     # Data preprocessing
â”‚   â”œâ”€â”€ features/                # Feature engineering
â”‚   â”‚   â””â”€â”€ build_features.py   # Feature creation
â”‚   â”œâ”€â”€ models/                  # Model code
â”‚   â”‚   â”œâ”€â”€ model.py            # Model definitions
â”‚   â”‚   â”œâ”€â”€ train.py            # Training logic
â”‚   â”‚   â”œâ”€â”€ evaluate.py         # Evaluation metrics
â”‚   â”‚   â””â”€â”€ predict.py          # Prediction logic
â”‚   â”œâ”€â”€ pipelines/              # ML pipelines
â”‚   â”‚   â”œâ”€â”€ training_pipeline.py    # Training workflow
â”‚   â”‚   â”œâ”€â”€ inference_pipeline.py   # Batch inference
â”‚   â”‚   â””â”€â”€ monitoring_pipeline.py  # Model monitoring
â”‚   â”œâ”€â”€ utils/                  # Utilities
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration
â”‚   â”‚   â”œâ”€â”€ logger.py          # Logging setup
â”‚   â”‚   â”œâ”€â”€ helpers.py         # Helper functions
â”‚   â”‚   â””â”€â”€ db_utils.py        # Database utilities
â”‚   â””â”€â”€ tests/                 # Tests
â”‚       â”œâ”€â”€ unit/             # Unit tests
â”‚       â”œâ”€â”€ integration/      # Integration tests
â”‚       â””â”€â”€ e2e/             # End-to-end tests
â”‚
â”œâ”€â”€ airflow/                   # Airflow orchestration
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ training_pipeline.py
â”‚   â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ mlflow/                    # MLflow setup
â”‚   â”œâ”€â”€ tracking/             # MLflow artifacts
â”‚   â”œâ”€â”€ docker/              # MLflow Docker config
â”‚   â””â”€â”€ mlflow_server.sh    # Startup script
â”‚
â”œâ”€â”€ configs/                  # Configuration files
â”‚   â”œâ”€â”€ training_config.yaml
â”‚   â”œâ”€â”€ inference_config.yaml
â”‚   â””â”€â”€ airflow_config.yaml
â”‚
â”œâ”€â”€ deployment/              # Deployment configs
â”‚   â”œâ”€â”€ dev/
â”‚   â”œâ”€â”€ sit/
â”‚   â”œâ”€â”€ uat/
â”‚   â””â”€â”€ prod/
â”‚
â”œâ”€â”€ models/                  # Model artifacts
â”‚   â”œâ”€â”€ production/
â”‚   â”œâ”€â”€ staging/
â”‚   â””â”€â”€ trained/
â”‚
â”œâ”€â”€ data/                   # Data storage
â”‚   â”œâ”€â”€ trainset/
â”‚   â””â”€â”€ testset/
â”‚
â”œâ”€â”€ experiments/           # Experimentation
â”‚   â”œâ”€â”€ notebooks/        # Jupyter notebooks
â”‚   â””â”€â”€ reports/         # Analysis reports
â”‚
â”œâ”€â”€ ci_cd/                # CI/CD pipelines
â”‚   â””â”€â”€ github_actions/
â”‚
â”œâ”€â”€ scripts/             # Utility scripts
â”‚   â”œâ”€â”€ run_training.sh
â”‚   â””â”€â”€ run_inference.sh
â”‚
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ setup.py           # Package setup
â”œâ”€â”€ Makefile          # Build automation
â””â”€â”€ README.md        # This file
```

## ğŸš€ Quick Start

### Installation

```bash
# Install package
pip install -e .

# Or just requirements
pip install -r requirements.txt
```

### Training

```bash
# Using Makefile
make train

# Or directly
python -m src.pipelines.training_pipeline --model-type random_forest --n-estimators 100

# Using script
./scripts/run_training.sh
```

### Inference

```bash
# Batch prediction
python -m src.pipelines.inference_pipeline \
    --input data/testset/test.csv \
    --output predictions.csv

# Using script
./scripts/run_inference.sh data/testset/test.csv predictions.csv
```

## ğŸ“Š MLflow Tracking

Start MLflow server:

```bash
# Using script
cd mlflow
./mlflow_server.sh

# Or directly
mlflow server --backend-store-uri sqlite:///mlflow/tracking/mlflow.db \
              --default-artifact-root ./mlflow/tracking/artifacts \
              --host 0.0.0.0 --port 5000
```

Access UI: http://localhost:5000

## ğŸ”„ Airflow Orchestration

Start Airflow (Astronomer):

```bash
astro dev start
```

Access UI: http://localhost:8080

DAG: `car_price_training_pipeline_v2`

## ğŸ³ Docker Deployment

```bash
# Development
make docker-up

# Or specific environment
cd deployment/dev
docker-compose -f docker-compose.dev.yml up

# Production
cd deployment/prod
docker-compose -f docker-compose.prod.yml up -d
```

## ğŸ§ª Testing

```bash
# All tests
make test

# Specific test suites
pytest src/tests/unit/ -v
pytest src/tests/integration/ -v
pytest src/tests/e2e/ -v
```

## ğŸ“ Configuration

All configurations are in `configs/`:

- `training_config.yaml` - Training parameters
- `inference_config.yaml` - Inference settings
- `airflow_config.yaml` - DAG configuration

## ğŸ” Monitoring

Monitor model performance:

```python
from src.pipelines.monitoring_pipeline import ModelMonitor

monitor = ModelMonitor()
report = monitor.generate_monitoring_report()
```

## ğŸ“ˆ Model Registry

Models are automatically registered to MLflow with versioning:

- **Production**: `models/production/model.pkl`
- **Staging**: `models/staging/`
- **Trained**: `models/trained/`

## ğŸ—„ï¸ Database

PostgreSQL stores evaluation metrics:

- Model runs and versions
- Performance metrics
- Data quality checks
- Alerts and monitoring

Connection: See `configs/` or environment variable `DATABASE_URL`

## ğŸ› ï¸ Development

```bash
# Install dev dependencies
pip install -e ".[dev]"

# Format code
make format

# Lint code
make lint

# Clean artifacts
make clean
```

## ğŸ“¦ CI/CD

GitHub Actions workflow: `.github/workflows/ml_pipeline.yml`

Triggers:
- Push to main/develop
- Pull requests
- Daily schedule (2 AM)
- Manual dispatch

## ğŸ” Environment Variables

```bash
export MLFLOW_TRACKING_URI=http://localhost:5000
export DATABASE_URL=postgresql://user:pass@localhost:5433/ml_evaluation
export ENV=development
```

## ğŸ“š API Usage

### Training API

```python
from src.models.train import ModelTrainer

trainer = ModelTrainer(model_type='random_forest')
results = trainer.train(n_estimators=100, max_depth=20)
```

### Prediction API

```python
from src.models.predict import ModelPredictor

predictor = ModelPredictor(stage='production')
prediction = predictor.predict({
    'year': 2020,
    'km': 50000,
    'fuel_type': 'Petrol',
    'seller_type': 'Dealer',
    'transmission': 'Manual'
})
```

## ğŸ—ï¸ Architecture

### Data Flow

```
Data Ingestion â†’ Validation â†’ Feature Engineering â†’ Preprocessing â†’ Model Training â†’ Evaluation â†’ Registration â†’ Deployment
```

### Components

1. **Data Layer**: Ingestion, validation, preprocessing
2. **Feature Layer**: Feature engineering and transformation
3. **Model Layer**: Training, evaluation, prediction
4. **Pipeline Layer**: Orchestration and workflow
5. **Infrastructure Layer**: MLflow, Airflow, Docker, Kubernetes

## ğŸ¤ Contributing

1. Create feature branch
2. Make changes
3. Run tests: `make test`
4. Format code: `make format`
5. Submit PR

## ğŸ“„ License

MIT License

## ğŸ‘¥ Team

ML Team - mailtopprakash01@gmail.com

## ğŸ”— Links

- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [Airflow Documentation](https://airflow.apache.org/docs/)
- [Project Deployment Guide](deployment/README.md)
