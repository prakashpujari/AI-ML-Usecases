{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What We will Learn\n",
    "\n",
    "1. S3 Buckets- Boto3\n",
    "2. Iam Roles and Users\n",
    "3. Complete Infrastructure of AWS Sagemaker-Training, Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup imports: sagemaker for training/deployment, boto3 for low-level AWS, pandas for data, os for env\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create a low-level SageMaker boto3 client for describing jobs/artifacts\n",
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "\n",
    "# Create a SageMaker Session object; provide a compatibility fallback for older versions\n",
    "try:\n",
    "    sess = sagemaker.Session()\n",
    "except Exception:\n",
    "    from sagemaker.core.helper.session_helper import Session as SageMakerSession\n",
    "    sess = SageMakerSession()\n",
    "\n",
    "# Get current AWS region and set bucket name used in examples\n",
    "region = sess.boto_session.region_name\n",
    "bucket = \"mobbucketsagemakerv1\"\n",
    "\n",
    "# Use `SAGEMAKER_INSTANCE_TYPE` env var to switch between local and AWS execution\n",
    "instance_type = os.environ.get(\"SAGEMAKER_INSTANCE_TYPE\", \"local\")\n",
    "print(\"Using bucket \" + bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the AWS region determined from the SageMaker session\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset into a DataFrame and show the first rows for inspection\n",
    "df = pd.read_csv(\"mob_price_classification_train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of the dataset (rows, columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column to detect nulls or data issues\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect class distribution for the target column 'price_range'\n",
    "df['price_range'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List feature columns in the dataset (will be used for modeling)\n",
    "features = list(df.columns)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the last column is the label/target and remove it from the features list\n",
    "label = features.pop(-1)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify remaining feature column names\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into feature matrix X and target vector y\n",
    "x = df[features]\n",
    "y = df[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split (85% train, 15% test) for local experimentation\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes of the train and test splits to confirm sizes\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct DataFrames with labels appended so we can write CSVs with the target column\n",
    "trainX = pd.DataFrame(X_train)\n",
    "trainX[label] = y_train\n",
    "\n",
    "testX = pd.DataFrame(X_test)\n",
    "testX[label] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example of the training DataFrame (features + label)\n",
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training and testing CSVs used by the training job or local runs\n",
    "trainX.to_csv(\"train-V-1.csv\", index=False)\n",
    "testX.to_csv(\"test-V-1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the bucket variable defined earlier (for verification)\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare S3 paths for training data when running on AWS; use file:// URIs for local runs\n",
    "sk_prefix = \"sagemaker/sklearn-mob-price-classification/sklearncontainer\"\n",
    "\n",
    "if not str(instance_type).startswith(\"local\"):\n",
    "    # Upload CSVs to S3 and capture the S3 URIs for the estimator\n",
    "    trainpath = sess.upload_data(path='train-V-1.csv', bucket=bucket, key_prefix=sk_prefix)\n",
    "    testpath = sess.upload_data(path='test-V-1.csv', bucket=bucket, key_prefix=sk_prefix)\n",
    "    print(trainpath)\n",
    "    print(testpath)\n",
    "else:\n",
    "    # local mode: use file:// paths for the estimator.fit fallback\n",
    "    trainpath = \"file://train-V-1.csv\"\n",
    "    testpath = \"file://test-V-1.csv\"\n",
    "    print(\"Local mode: using\", trainpath, testpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script used by AWS Sagemaker To Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile script.py\n",
    "\"\"\"\n",
    "Training script for a RandomForest classifier used with SageMaker/local runs.\n",
    "\n",
    "This module trains a RandomForest model using CSV train/test files, saves\n",
    "the trained model to `model.joblib` under the provided model directory, and\n",
    "prints basic evaluation metrics on the test set.\n",
    "\n",
    "The script expects the following (can be provided via SageMaker env vars):\n",
    "- `SM_MODEL_DIR` -> model output directory\n",
    "- `SM_CHANNEL_TRAIN` -> path to training data channel\n",
    "- `SM_CHANNEL_TEST` -> path to testing data channel\n",
    "\n",
    "Run as a script for local testing, or used by SageMaker during training.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "import sklearn\n",
    "import joblib\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load and return the trained model from `model_dir`.\n",
    "\n",
    "    This function follows the SageMaker inference convention where the\n",
    "    serving/container runtime calls `model_fn` to deserialize the model.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str): Directory where `model.joblib` is stored.\n",
    "\n",
    "    Returns:\n",
    "        sklearn estimator: The deserialized model object.\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parse input arguments (hyperparameters and channel locations)\n",
    "    print(\"[Info] Extracting arguments\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters for the RandomForest model\n",
    "    parser.add_argument(\"--n_estimators\", type=int, default=100, help=\"Number of trees in the forest\")\n",
    "    parser.add_argument(\"--random_state\", type=int, default=0, help=\"Random seed for reproducibility\")\n",
    "\n",
    "    # Directories: model output and data channels (SageMaker style env vars)\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"), help=\"Model output directory\")\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"), help=\"Training data channel path\")\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"), help=\"Testing data channel path\")\n",
    "    parser.add_argument(\"--train-file\", type=str, default=\"train-V-1.csv\", help=\"Training CSV file name\")\n",
    "    parser.add_argument(\"--test-file\", type=str, default=\"test-V-1.csv\", help=\"Testing CSV file name\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Report versions for reproducibility/debugging\n",
    "    print(\"SKLearn Version: \", sklearn.__version__)\n",
    "    print(\"Joblib Version: \", joblib.__version__)\n",
    "\n",
    "    # Read CSV data from provided channels\n",
    "    print(\"[INFO] Reading data\")\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    # Last column is assumed to be the label; all preceding columns are features\n",
    "    features = list(train_df.columns)\n",
    "    label = features.pop(-1)\n",
    "\n",
    "    print(\"Building training and testing datasets\")\n",
    "    X_train = train_df[features]\n",
    "    X_test = test_df[features]\n",
    "    y_train = train_df[label]\n",
    "    y_test = test_df[label]\n",
    "\n",
    "    # Print dataset summary information\n",
    "    print('Column order: ')\n",
    "    print(features)\n",
    "    print()\n",
    "    print(\"Label column is: \", label)\n",
    "    print()\n",
    "\n",
    "    print(\"Data Shape: \")\n",
    "    print(\"---- SHAPE OF TRAINING DATA (rows, cols) ----\")\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(\"---- SHAPE OF TESTING DATA (rows, cols) ----\")\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "    # Initialize and train the RandomForest model\n",
    "    print(\"Training RandomForest Model ....\")\n",
    "    model = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.random_state,\n",
    "                                   verbose=2, n_jobs=1)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the trained model for later inference\n",
    "    model_path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, model_path)\n",
    "    print(\"Model saved at \" + model_path)\n",
    "\n",
    "    # Evaluate on the test set and print metrics\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    test_rep = classification_report(y_test, y_pred_test)\n",
    "\n",
    "    print()\n",
    "    print(\"---- METRICS RESULTS FOR TESTING DATA ----\")\n",
    "    print(\"Total Rows are: \", X_test.shape[0])\n",
    "    print('[TESTING] Model Accuracy is: ', test_acc)\n",
    "    print('[TESTING] Testing Report: ')\n",
    "    print(test_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Sagemaker Entry Point To Execute the Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an SKLearn estimator that will use `script.py` as the entry point\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "import os\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "# Use local mode by default for development/testing. To run on AWS change env var SAGEMAKER_INSTANCE_TYPE\n",
    "instance_type = os.environ.get(\"SAGEMAKER_INSTANCE_TYPE\", \"local\")\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"script.py\",\n",
    "    role=\"arn:aws:iam::411715192815:role/sagemakeraccess\",\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"RF-custom-sklearn\",\n",
    "    hyperparameters={\n",
    "        \"n_estimators\": 100,\n",
    "        \"random_state\": 0\n",
    "    },\n",
    "    # Spot instances not supported for local mode\n",
    "    use_spot_instance=(False if instance_type.startswith(\"local\") else True),\n",
    "    max_run=3600\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the training job. Use local file URIs if running in local mode, otherwise pass S3 paths\n",
    "if sklearn_estimator.instance_type and str(sklearn_estimator.instance_type).startswith(\"local\"):\n",
    "    fit_inputs = {\"train\": \"file://train-V-1.csv\", \"test\": \"file://test-V-1.csv\"}\n",
    "else:\n",
    "    fit_inputs = {\"train\": trainpath, \"test\": testpath}\n",
    "\n",
    "# Start the training job and wait for completion\n",
    "sklearn_estimator.fit(fit_inputs, wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get the model from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the trained model artifact. For local training, package and upload the model to S3\n",
    "sklearn_estimator.latest_training_job.wait(logs=\"None\")\n",
    "\n",
    "if str(instance_type).startswith(\"local\"):\n",
    "    import glob, tempfile, tarfile\n",
    "    import boto3\n",
    "    import os\n",
    "\n",
    "    # attempt to find the saved model produced by the training container\n",
    "    matches = glob.glob(\"**/model.joblib\", recursive=True)\n",
    "    if not matches:\n",
    "        matches = glob.glob(os.path.join(tempfile.gettempdir(), \"**/model.joblib\"), recursive=True)\n",
    "    if not matches:\n",
    "        raise Exception(\"model.joblib not found after local training\")\n",
    "    model_path = matches[0]\n",
    "\n",
    "    # package the model into a tar.gz expected by SageMaker\n",
    "    tar_path = os.path.join(os.getcwd(), \"model.tar.gz\")\n",
    "    with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "        tar.add(model_path, arcname=\"model.joblib\")\n",
    "\n",
    "    # upload to S3 so we can create a model for deployment\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    # ensure sk_prefix exists\n",
    "    try:\n",
    "        sk_prefix\n",
    "    except NameError:\n",
    "        sk_prefix = \"sagemaker/sklearn-mob-price-classification/sklearncontainer\"\n",
    "    s3_key = f\"{sk_prefix}/model.tar.gz\"\n",
    "    s3.upload_file(tar_path, bucket, s3_key)\n",
    "    artifact = f\"s3://{bucket}/{s3_key}\"\n",
    "    print(\"Uploaded local model to\", artifact)\n",
    "else:\n",
    "    artifact = sm_boto3.describe_training_job(\n",
    "        TrainingJobName=sklearn_estimator.latest_training_job.name\n",
    "    )[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "# The variable `artifact` contains the S3 URI for the trained model\n",
    "artifact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the artifact S3 URI pointing to the trained model\n",
    "artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Model For Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SKLearnModel object for deployment using the trained artifact\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name = \"Custom-sklearn-model-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model = SKLearnModel(\n",
    "    name=model_name,\n",
    "    model_data=artifact,\n",
    "    role=\"arn:aws:iam::411715192815:role/sagemakeraccess\",\n",
    "    entry_point=\"script.py\",\n",
    "    framework_version=FRAMEWORK_VERSION\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model object prepared for deployment\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to an endpoint (this will create an endpoint in AWS unless running in local mode)\n",
    "endpoint_name = \"Custom-sklearn-model-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"EndpointName={}\".format(endpoint_name))\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    endpoint_name=endpoint_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the predictor object which can be used to make real-time inferences\n",
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first two rows of features from test set to prepare sample payload for prediction\n",
    "testX[features][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the deployed endpoint for two sample rows from the test set\n",
    "print(predictor.predict(testX[features][:2].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the deployed endpoint to avoid incurring charges in AWS\n",
    "sm_boto3.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
